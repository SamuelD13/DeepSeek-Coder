{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:18:41.319262Z",
     "iopub.status.busy": "2024-08-18T15:18:41.318821Z",
     "iopub.status.idle": "2024-08-18T15:18:41.605662Z",
     "shell.execute_reply": "2024-08-18T15:18:41.605147Z",
     "shell.execute_reply.started": "2024-08-18T15:18:41.319241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "with open('key.txt', 'r') as file:\n",
    "    key = file.readline().strip()\n",
    "\n",
    "login(token=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:18:41.607077Z",
     "iopub.status.busy": "2024-08-18T15:18:41.606683Z",
     "iopub.status.idle": "2024-08-18T15:18:44.497268Z",
     "shell.execute_reply": "2024-08-18T15:18:44.496845Z",
     "shell.execute_reply.started": "2024-08-18T15:18:41.607059Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 15:18:43.009413: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-18 15:18:43.009480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-18 15:18:43.010626: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-18 15:18:43.017009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-18 15:18:43.727129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "SEED = 10107\n",
    "set_seed(SEED)\n",
    "\n",
    "MODEL = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "DATASET = \"google/code_x_glue_ct_code_to_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:18:44.500445Z",
     "iopub.status.busy": "2024-08-18T15:18:44.500122Z",
     "iopub.status.idle": "2024-08-18T15:18:44.516558Z",
     "shell.execute_reply": "2024-08-18T15:18:44.516149Z",
     "shell.execute_reply.started": "2024-08-18T15:18:44.500427Z"
    }
   },
   "outputs": [],
   "source": [
    "#This set of utilities function and classes was imported \n",
    "#from https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/finetune/finetune_deepseekcoder.py\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "EOT_TOKEN = \"<|EOT|>\"\n",
    "\n",
    "def build_instruction_prompt(instruction: str):\n",
    "    return '''\n",
    "You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
    "### Instruction:\n",
    "{}\n",
    "### Response:\n",
    "'''.format(instruction.strip()).lstrip()\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = [torch.tensor(x) for x in input_ids]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = [torch.tensor(x) for x in labels]\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        \n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "def train_tokenize_function(examples, tokenizer):\n",
    "    sources = [\n",
    "        build_instruction_prompt(instruction)\n",
    "        for instruction in examples['instruction']\n",
    "    ]\n",
    "    targets = [f\"{output}\\n{EOT_TOKEN}\" for output in examples['output']]\n",
    "    data_dict = preprocess(sources, targets, tokenizer)\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:18:44.518056Z",
     "iopub.status.busy": "2024-08-18T15:18:44.517617Z",
     "iopub.status.idle": "2024-08-18T15:18:44.522375Z",
     "shell.execute_reply": "2024-08-18T15:18:44.521938Z",
     "shell.execute_reply.started": "2024-08-18T15:18:44.518040Z"
    }
   },
   "outputs": [],
   "source": [
    "#This set of utilities function and classes was imported \n",
    "#from https://github.com/bigcode-project/bigcode-evaluation-harness/blob/main/bigcode_eval/tasks/codexglue_code_to_text.py\n",
    "\n",
    "import re\n",
    "\n",
    "TRIPLE_QUOTE = '\"\"\"'\n",
    "SINGLE_TRIPLE_QUOTE = \"'''\"\n",
    "SUFFIX_PROMPT = '\\n\"\"\" The goal of this function is to:\\n'\n",
    "\n",
    "def standardize_docstring_prompt(prefix):\n",
    "    \"\"\"Strips any existing docstring delimiters from the prompt prefix\n",
    "    and adds our own delimiter (triple quote) and whitespace.\n",
    "    Note an edge cases being handled here:\n",
    "    - codexglue docstring text sometimes contains the docstring delimiters, inconsistently\n",
    "\n",
    "    source: InCoder evaluation code https://github.com/dpfried/lm-evaluation-harness/\n",
    "    \"\"\"\n",
    "\n",
    "    for delim in [TRIPLE_QUOTE, SINGLE_TRIPLE_QUOTE]:\n",
    "        if delim in prefix:\n",
    "            prefix = prefix[: prefix.index(delim)]\n",
    "            break\n",
    "\n",
    "    single_single_quote_with_trailing_spaces = re.compile(r'[^\\'\"][\\']\\s*$')\n",
    "    if single_single_quote_with_trailing_spaces.search(prefix):\n",
    "        prefix = prefix[\n",
    "            : single_single_quote_with_trailing_spaces.search(prefix).start()\n",
    "        ]\n",
    "\n",
    "    single_double_quote_with_trailing_spaces = re.compile(r'[^\\'\"][\"]\\s*$')\n",
    "    if single_double_quote_with_trailing_spaces.search(prefix):\n",
    "        prefix = prefix[\n",
    "            : single_double_quote_with_trailing_spaces.search(prefix).start()\n",
    "        ]\n",
    "\n",
    "    prefix += TRIPLE_QUOTE\n",
    "    return prefix\n",
    "\n",
    "def remove_docstring(doc):\n",
    "    \"\"\"Generate prompts for Code to text benchmark (documentation generation)\n",
    "    Prompt = full function body (withoout the docstring) + '\\n[Delimiter] The goal of this function is to:\\n'\n",
    "    where delimiter is  \\\"\"\" for python, =begin for ruby and /* for the rest (see SUFFIX_PROMPT).\n",
    "    :param doc: dict[str: str])\n",
    "    \"\"\"\n",
    "    code = doc[\"code\"]\n",
    "    text = doc[\"docstring\"]\n",
    "    prompt_prefix = code[: code.index(text)]\n",
    "    prompt_prefix = standardize_docstring_prompt(prompt_prefix)\n",
    "    prompt_suffix = code[code.index(text) + len(text) :]\n",
    "    prompt_suffix = prompt_suffix.replace(TRIPLE_QUOTE, \"\")\n",
    "    prompt_suffix = prompt_suffix.replace(SINGLE_TRIPLE_QUOTE, \"\")\n",
    "\n",
    "    prompt_prefix = prompt_prefix.strip().removesuffix(TRIPLE_QUOTE)\n",
    "    prompt_prefix = prompt_prefix.strip().removesuffix(SINGLE_TRIPLE_QUOTE)\n",
    "    prompt = prompt_prefix + prompt_suffix + SUFFIX_PROMPT\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:18:44.523073Z",
     "iopub.status.busy": "2024-08-18T15:18:44.522929Z",
     "iopub.status.idle": "2024-08-18T15:18:48.694600Z",
     "shell.execute_reply": "2024-08-18T15:18:48.694193Z",
     "shell.execute_reply.started": "2024-08-18T15:18:44.523060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n",
      "    num_rows: 251820\n",
      "})\n",
      "Dataset({\n",
      "    features: ['output', 'instruction'],\n",
      "    num_rows: 15000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "dataset = load_dataset(\n",
    "    DATASET,\n",
    "    'python',\n",
    "    split=\"train\",\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "NB_SAMPLES = 15000\n",
    "\n",
    "dataset = dataset.shuffle(seed=SEED)\n",
    "dataset = dataset.select(range(NB_SAMPLES))\n",
    "\n",
    "cleaned_code = [\n",
    "    remove_docstring(sample) for sample in dataset\n",
    "]\n",
    "\n",
    "column_names = dataset.column_names\n",
    "column_names.remove(\"docstring\")\n",
    "    \n",
    "dataset = dataset.rename_column(\"docstring\", \"output\")\n",
    "dataset = dataset.remove_columns(column_names)\n",
    "dataset = dataset.add_column('instruction', cleaned_code)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:18:48.695843Z",
     "iopub.status.busy": "2024-08-18T15:18:48.695501Z",
     "iopub.status.idle": "2024-08-18T15:18:56.313116Z",
     "shell.execute_reply": "2024-08-18T15:18:56.312536Z",
     "shell.execute_reply.started": "2024-08-18T15:18:48.695825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples to remove:  318\n",
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 14682\n",
      "})\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 11745\n",
      "})\n",
      "Sample 4 of the training set: [32013, 2042, 417, 274, 20926, 14244, 20391, 11, 26696, 254, 20676, 8041, 74, 339, 8589, 2008, 11, 6908, 457, 20676, 8041, 74, 7958, 11, 285, 340, 885, 3495, 4301, 4512, 276, 4531, 8214, 13, 1487, 4636, 2223, 13143, 4301, 11, 5411, 285, 13936, 4447, 11, 285, 746, 2159, 12, 13517, 250, 8214, 4301, 11, 340, 540, 20857, 276, 3495, 13, 185, 13518, 3649, 3475, 25, 185, 1551, 12054, 7, 1180, 11, 3447, 62, 9523, 1772, 185, 294, 185, 436, 3579, 405, 3270, 2933, 4581, 13, 3154, 62, 22052, 7, 1180, 11, 3447, 62, 9523, 8, 185, 436, 967, 15224, 12636, 7, 4008, 8, 185, 23984, 428, 6206, 280, 437, 1155, 317, 276, 25, 185, 13518, 21289, 25, 185, 4317, 81, 1804, 4814, 365, 756, 3029, 473, 7122, 15224, 12636, 285, 7579, 245, 756, 15224, 12636, 2148, 13, 185, 185, 436, 1191, 2280, 3447, 62, 9523, 25, 15224, 4581, 365, 4814, 276, 12054, 254, 1642, 3029, 13, 185, 185, 436, 1191, 2125, 25, 245, 756, 15224, 12636, 2148, 13, 185, 32021], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4317, 81, 1804, 4814, 365, 756, 3029, 473, 7122, 15224, 12636, 285, 7579, 245, 756, 15224, 12636, 2148, 13, 185, 185, 436, 1191, 2280, 3447, 62, 9523, 25, 15224, 4581, 365, 4814, 276, 12054, 254, 1642, 3029, 13, 185, 185, 436, 1191, 2125, 25, 245, 756, 15224, 12636, 2148, 13, 185, 32021].\n",
      "Sample 4 of the training set: <beginofsentence>You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
      "### Instruction:\n",
      "def override(self, config_params):\n",
      "        \n",
      "        map = StringValueMap.from_maps(self, config_params)\n",
      "        return ConfigParams(map)\n",
      "\"\"\" The goal of this function is to:\n",
      "### Response:\n",
      "Overrides parameters with new values from specified ConfigParams and returns a new ConfigParams object.\n",
      "\n",
      "        :param config_params: ConfigMap with parameters to override the current values.\n",
      "\n",
      "        :return: a new ConfigParams object.\n",
      "<|EOT|>.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    train_tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size = 1000,\n",
    "    remove_columns=dataset.column_names,\n",
    "    fn_kwargs={ \"tokenizer\": tokenizer }\n",
    ")\n",
    "\n",
    "sequence_length = []\n",
    "for seq in tokenized_dataset['input_ids']:\n",
    "    sequence_length.append(len(seq))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(sequence_length, bins=50)\n",
    "plt.title(\"Length of samples distribution\")\n",
    "plt.show()\n",
    "\n",
    "long_sequences_indices = [i for i, example in enumerate(tokenized_dataset) if len(example['input_ids']) > 1024]\n",
    "print('Number of samples to remove: ', len(long_sequences_indices))\n",
    "\n",
    "filtered_tokenized_dataset = tokenized_dataset.filter(lambda example, idx: idx not in long_sequences_indices, with_indices=True)\n",
    "print(filtered_tokenized_dataset)\n",
    "\n",
    "sequence_length = []\n",
    "for seq in filtered_tokenized_dataset['input_ids']:\n",
    "    sequence_length.append(len(seq))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(sequence_length, bins=50)\n",
    "plt.title(\"Length of samples distribution\")\n",
    "plt.show()\n",
    "\n",
    "split_dataset = filtered_tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_data = split_dataset['train']\n",
    "valid_data = split_dataset['test']\n",
    "\n",
    "print(train_data)\n",
    "index=4\n",
    "print(f\"Sample {index} of the training set: {train_data[index]['input_ids']}, {train_data[index]['labels']}.\")\n",
    "print(f\"Sample {index} of the training set: {tokenizer.decode(list(train_data[index]['input_ids']))}.\")\n",
    "\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "data_module = dict(train_dataset=train_data, eval_dataset=valid_data, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:18:56.314477Z",
     "iopub.status.busy": "2024-08-18T15:18:56.314123Z",
     "iopub.status.idle": "2024-08-18T15:19:06.236292Z",
     "shell.execute_reply": "2024-08-18T15:19:06.235789Z",
     "shell.execute_reply.started": "2024-08-18T15:18:56.314460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287ff1b600904fd18097634e9e8ec40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=getattr(torch, \"bfloat16\"),\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    load_in_8bit=False,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:19:06.237074Z",
     "iopub.status.busy": "2024-08-18T15:19:06.236902Z",
     "iopub.status.idle": "2024-08-18T15:19:06.882646Z",
     "shell.execute_reply": "2024-08-18T15:19:06.882208Z",
     "shell.execute_reply.started": "2024-08-18T15:19:06.237058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o_proj', 'gate_proj', 'q_proj', 'v_proj', 'up_proj', 'down_proj', 'k_proj']\n",
      "trainable params: 19,988,480 || all params: 6,760,501,248 || trainable%: 0.2956656506189288\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model, bit4=True, bit8=True):\n",
    "    LinearModule = torch.nn.Linear\n",
    "    if bit4:\n",
    "        LinearModule = bnb.nn.Linear4bit\n",
    "    if bit8:\n",
    "        LinearModule = bnb.nn.Linear8bitLt\n",
    "        \n",
    "    module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LinearModule):\n",
    "            if \"lm_head\" in module_names:\n",
    "                continue\n",
    "            names = name.split(\".\")\n",
    "            module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    \n",
    "    return list(module_names)\n",
    "\n",
    "target_modules = find_all_linear_names(model, bit4=True, bit8=False)\n",
    "print(target_modules)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:19:06.883777Z",
     "iopub.status.busy": "2024-08-18T15:19:06.883622Z",
     "iopub.status.idle": "2024-08-18T15:19:08.179585Z",
     "shell.execute_reply": "2024-08-18T15:19:08.179158Z",
     "shell.execute_reply.started": "2024-08-18T15:19:06.883762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamueld\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "# Set up wandb\n",
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "wandb.login(key=\"22943c48738b2f0aa5a6b37af531509b75a16960\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     loss = F.cross_entropy(logits, labels).item()\n",
    "#     return {\"loss\": loss}\n",
    "\n",
    "# def preprocess_logits_for_metrics(logits, labels):\n",
    "#     return logits\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"]=\"llm-finetune\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T15:19:08.181552Z",
     "iopub.status.busy": "2024-08-18T15:19:08.181185Z",
     "iopub.status.idle": "2024-08-18T20:58:18.788563Z",
     "shell.execute_reply": "2024-08-18T20:58:18.786205Z",
     "shell.execute_reply.started": "2024-08-18T15:19:08.181535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240818_151908-gnmd3u96</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samueld/llm-finetune/runs/gnmd3u96' target=\"_blank\">polished-dust-101</a></strong> to <a href='https://wandb.ai/samueld/llm-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samueld/llm-finetune' target=\"_blank\">https://wandb.ai/samueld/llm-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samueld/llm-finetune/runs/gnmd3u96' target=\"_blank\">https://wandb.ai/samueld/llm-finetune/runs/gnmd3u96</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='369' max='2944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 369/2944 1:13:19 < 8:34:25, 0.08 it/s, Epoch 1/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='736' max='5880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 736/5880 1:06:06 < 7:43:17, 0.19 it/s, Epoch 1/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1470' max='11752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1470/11752 1:01:19 < 7:09:31, 0.40 it/s, Epoch 1/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2938' max='23496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2938/23496 1:00:00 < 7:00:10, 0.82 it/s, Epoch 1/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5874' max='46984' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5874/46984 1:07:20 < 7:51:24, 1.45 it/s, Epoch 1/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1237' max='93960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1237/93960 09:44 < 12:11:56, 2.11 it/s, Epoch 0.11/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39mtraining_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_module)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:1546\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/memory.py:136\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2748\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2747\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2748\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:659\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:647\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:659\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:647\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 647 (9 times), autocast_decorator.<locals>.decorate_autocast at line 16 (9 times), convert_outputs_to_fp32.<locals>.forward at line 659 (9 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:659\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:647\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1003\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m    994\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    995\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1001\u001b[0m         )\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:107\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:912\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    909\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[idx] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 912\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    923\u001b[0m         hidden_states,\n\u001b[1;32m    924\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    929\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:451\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    454\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    455\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:230\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    227\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 230\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:686\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    685\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:281\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    280\u001b[0m lora_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[active_adapter]\n\u001b[0;32m--> 281\u001b[0m lora_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_B\u001b[49m[active_adapter]\n\u001b[1;32m    282\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_dropout[active_adapter]\n\u001b[1;32m    283\u001b[0m scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling[active_adapter]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1684\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f490c420490>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f490c3c8110, execution_count=10 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 7f4ab0644810, raw_cell=\"from transformers import Trainer, TrainingArgument..\" store_history=True silent=False shell_futures=True cell_id=c4f7bab5-2bae-4185-bb1a-589697f59360> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "MAX_EPOCHS = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 400\n",
    "LOG_FREQ = 10\n",
    "BATCH_SIZE = 2048\n",
    "OUTPUT_DIR = \"deepseek6.7-summarize-coder\"\n",
    "SCHEDULER = \"cosine\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"Sam137/{OUTPUT_DIR}\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to = \"wandb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    logging_steps=LOG_FREQ,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.95,\n",
    "    lr_scheduler_type=SCHEDULER,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    push_to_hub=True,\n",
    "    auto_find_batch_size=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, **data_module)\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T20:58:22.367905Z",
     "iopub.status.busy": "2024-08-18T20:58:22.367395Z",
     "iopub.status.idle": "2024-08-18T20:58:31.293964Z",
     "shell.execute_reply": "2024-08-18T20:58:31.293529Z",
     "shell.execute_reply.started": "2024-08-18T20:58:22.367885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f490c420490>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f48f55eb0d0, raw_cell=\"wandb.finish()\" store_history=True silent=False shell_futures=True cell_id=8e9cc364-23af-45b0-8b8a-f3a7e93a073b>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.1</td></tr><tr><td>train/global_step</td><td>1230</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.6589</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-dust-101</strong> at: <a href='https://wandb.ai/samueld/llm-finetune/runs/gnmd3u96' target=\"_blank\">https://wandb.ai/samueld/llm-finetune/runs/gnmd3u96</a><br/> View job at <a href='https://wandb.ai/samueld/llm-finetune/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjQxNDc2NjM2Mw==/version_details/v3' target=\"_blank\">https://wandb.ai/samueld/llm-finetune/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjQxNDc2NjM2Mw==/version_details/v3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240818_151908-gnmd3u96/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
