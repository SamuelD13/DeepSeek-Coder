{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T11:15:06.559264Z",
     "iopub.status.busy": "2024-08-20T11:15:06.558784Z",
     "iopub.status.idle": "2024-08-20T11:15:06.874404Z",
     "shell.execute_reply": "2024-08-20T11:15:06.873840Z",
     "shell.execute_reply.started": "2024-08-20T11:15:06.559234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "with open('key.txt', 'r') as file:\n",
    "    key = file.readline().strip()\n",
    "\n",
    "login(token=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T11:15:06.875590Z",
     "iopub.status.busy": "2024-08-20T11:15:06.875383Z",
     "iopub.status.idle": "2024-08-20T11:15:12.796520Z",
     "shell.execute_reply": "2024-08-20T11:15:12.795868Z",
     "shell.execute_reply.started": "2024-08-20T11:15:06.875575Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 11:15:09.369134: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-20 11:15:09.369261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-20 11:15:09.447992: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-20 11:15:09.612848: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-20 11:15:10.986262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "SEED = 10107\n",
    "set_seed(SEED)\n",
    "\n",
    "MODEL = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "DATASET = \"Sam137/CompareEval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T11:15:12.797484Z",
     "iopub.status.busy": "2024-08-20T11:15:12.797154Z",
     "iopub.status.idle": "2024-08-20T11:15:12.839108Z",
     "shell.execute_reply": "2024-08-20T11:15:12.838492Z",
     "shell.execute_reply.started": "2024-08-20T11:15:12.797467Z"
    }
   },
   "outputs": [],
   "source": [
    "#This set of utilities function and classes was imported \n",
    "#from https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/finetune/finetune_deepseekcoder.py\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "EOT_TOKEN = \"<|EOT|>\"\n",
    "\n",
    "def build_instruction_prompt(instruction: str):\n",
    "    return '''\n",
    "You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
    "### Instruction:\n",
    "{}\n",
    "### Response:\n",
    "'''.format(instruction.strip()).lstrip()\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = [torch.tensor(x) for x in input_ids]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = [torch.tensor(x) for x in labels]\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        \n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "def train_tokenize_function(examples, tokenizer):\n",
    "    sources = [\n",
    "        build_instruction_prompt(instruction)\n",
    "        for instruction in examples['instruction']\n",
    "    ]\n",
    "    targets = [f\"{output}\\n{EOT_TOKEN}\" for output in examples['output']]\n",
    "    data_dict = preprocess(sources, targets, tokenizer)\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T11:15:12.840496Z",
     "iopub.status.busy": "2024-08-20T11:15:12.840326Z",
     "iopub.status.idle": "2024-08-20T11:15:20.251558Z",
     "shell.execute_reply": "2024-08-20T11:15:20.250776Z",
     "shell.execute_reply.started": "2024-08-20T11:15:12.840481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70868934a6214f269265521c56a24d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24870060b48547d192e58e8088c516aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcc11fd8e52460792db69881712eca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/123M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5552edf703a46608766e5502a1ad75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011951495fd84040956215c59ca911f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/datasets/download/streaming_download_manager.py:765: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['commit', 'old_file', 'new_file', 'old_contents', 'new_contents', 'subject', 'message', 'lang', 'license', 'repos'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e313dece0f36465b97cc1705a4450962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['output', 'instruction'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "dataset = load_dataset(\n",
    "    DATASET,\n",
    "    data_files = [\"commitpack.csv\"],\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "NB_SAMPLES = 5000\n",
    "\n",
    "dataset = dataset.shuffle(seed=SEED)\n",
    "dataset = dataset.select(range(NB_SAMPLES))\n",
    "print(dataset)\n",
    "\n",
    "column_names = dataset.column_names\n",
    "column_names.remove(\"subject\")\n",
    "\n",
    "def build_prompt(first_code: str, second_code: str):\n",
    "    return f\"You will be given two Python code snippets: one is the original version, and the other is the updated version. Your task is to provide a clear, concise, and accurate short description of the update that was made in the updated version. Now, here are the original and updated code snippets for you to analyze:\\nOriginal code:\\n{first_code}\\nUpdated code:\\n{second_code}\\n\"\n",
    "\n",
    "def build_instruction(example):\n",
    "    example['instruction'] = build_prompt(example['old_contents'], example['new_contents'])\n",
    "\n",
    "instructions = [\n",
    "    build_prompt(old, new) for old, new in zip(dataset['old_contents'], dataset['new_contents'])\n",
    "]\n",
    "    \n",
    "dataset = dataset.add_column('instruction', instructions)\n",
    "dataset = dataset.rename_column(\"subject\", \"output\")\n",
    "dataset = dataset.remove_columns(column_names)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T11:15:20.252677Z",
     "iopub.status.busy": "2024-08-20T11:15:20.252346Z",
     "iopub.status.idle": "2024-08-20T11:15:43.526804Z",
     "shell.execute_reply": "2024-08-20T11:15:43.526307Z",
     "shell.execute_reply.started": "2024-08-20T11:15:20.252660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06794d89ef324107a9d0778a80c3107c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0098914e5c40b19481dd2ce7613fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d824aff759614072b07bd843c5e51560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 4000\n",
      "})\n",
      "Sample 4 of the training set: [32013, 2042, 417, 274, 20926, 14244, 20391, 11, 26696, 254, 20676, 8041, 74, 339, 8589, 2008, 11, 6908, 457, 20676, 8041, 74, 7958, 11, 285, 340, 885, 3495, 4301, 4512, 276, 4531, 8214, 13, 1487, 4636, 2223, 13143, 4301, 11, 5411, 285, 13936, 4447, 11, 285, 746, 2159, 12, 13517, 250, 8214, 4301, 11, 340, 540, 20857, 276, 3495, 13, 185, 13518, 3649, 3475, 25, 185, 2042, 540, 330, 2017, 979, 13003, 2974, 4494, 12111, 1542, 25, 629, 317, 254, 3620, 2615, 11, 285, 254, 746, 317, 254, 7992, 2615, 13, 4506, 5256, 317, 276, 2764, 245, 3650, 11, 3180, 1007, 11, 285, 10370, 2567, 6413, 280, 254, 3967, 344, 438, 1396, 279, 254, 7992, 2615, 13, 4195, 11, 1283, 417, 254, 3620, 285, 7992, 2974, 4494, 12111, 1542, 327, 340, 276, 17869, 25, 185, 26835, 2974, 25, 185, 2, 567, 9, 12, 25419, 25, 31979, 12, 23, 567, 9, 12, 185, 185, 29430, 29430, 7113, 3576, 13518, 185, 2, 997, 3117, 1753, 317, 692, 280, 254, 6771, 85, 529, 2299, 11, 6486, 1615, 28203, 85, 529, 13, 2156, 7936, 185, 2, 804, 317, 7974, 1089, 254, 207, 18, 12, 1982, 1029, 380, 6593, 10420, 11, 1016, 440, 19248, 22451, 2770, 185, 29430, 29430, 7113, 3576, 13518, 185, 1892, 1181, 4016, 372, 21807, 185, 185, 3154, 276, 22708, 529, 13, 1112, 1659, 7050, 1920, 11, 13265, 7190, 11, 31851, 11, 422, 8624, 185, 185, 1892, 276, 22708, 529, 13, 10518, 185, 185, 3154, 353, 15142, 1659, 353, 15142, 6635, 2714, 185, 185, 185, 2176, 461, 2990, 88, 7190, 7, 6860, 7190, 1772, 185, 185, 315, 1171, 6412, 6310, 185, 315, 972, 1753, 62, 2139, 10942, 185, 436, 967, 7050, 1920, 1497, 11835, 23883, 10042, 4797, 1183, 12036, 77, 4016, 17641, 185, 185, 185, 2176, 461, 2990, 88, 14830, 7, 14830, 11, 461, 2990, 88, 7190, 1772, 185, 185, 315, 972, 3697, 7, 1180, 11, 3076, 11, 1189, 62, 4534, 1772, 185, 436, 1189, 405, 276, 22708, 529, 13, 10518, 13, 703, 62, 2482, 7, 2448, 62, 4534, 8, 185, 436, 365, 1714, 7, 3261, 11, 440, 86, 65, 2456, 372, 267, 25, 185, 655, 21807, 13, 13766, 7, 69, 11, 1189, 8, 185, 185, 185, 2176, 461, 2990, 88, 13105, 7, 13105, 11, 461, 2990, 88, 7190, 1772, 185, 185, 315, 972, 1272, 7, 1180, 11, 3076, 1772, 185, 436, 365, 1714, 7, 3261, 11, 440, 15686, 2456, 372, 267, 25, 185, 655, 1189, 405, 21807, 13, 1768, 7, 69, 8, 185, 185, 436, 562, 10405, 7, 2448, 13, 15148, 8, 2069, 4084, 18, 25, 185, 655, 967, 353, 15142, 6635, 2714, 822, 185, 185, 436, 3310, 62, 2448, 405, 353, 15142, 6635, 2714, 822, 185, 436, 334, 87, 11, 320, 11, 1670, 8, 405, 1189, 13, 15148, 185, 185, 436, 3310, 62, 2448, 13, 2964, 27219, 7, 15, 11, 207, 15, 11, 207, 15, 8, 185, 436, 3310, 62, 2448, 13, 2964, 7592, 5015, 7, 16, 11, 207, 16, 11, 207, 16, 8, 185, 436, 3310, 62, 2448, 13, 2964, 6500, 289, 7, 15, 11, 1371, 567, 207, 16, 11, 207, 15, 11, 320, 567, 207, 16, 11, 207, 15, 11, 1670, 567, 207, 16, 8, 185, 436, 276, 22708, 529, 13, 10518, 13, 1113, 62, 2482, 7, 5468, 62, 2448, 11, 1189, 8, 185, 185, 436, 967, 3310, 62, 2448, 185, 185, 27962, 2974, 25, 185, 2, 567, 9, 12, 25419, 25, 31979, 12, 23, 567, 9, 12, 185, 185, 29430, 29430, 7113, 3576, 13518, 185, 2, 997, 3117, 1753, 317, 692, 280, 254, 6771, 85, 529, 2299, 11, 6486, 1615, 28203, 85, 529, 13, 2156, 7936, 185, 2, 804, 317, 7974, 1089, 254, 207, 18, 12, 1982, 1029, 380, 6593, 10420, 11, 1016, 440, 19248, 22451, 2770, 185, 29430, 29430, 7113, 3576, 13518, 185, 1892, 1181, 4016, 372, 21807, 185, 185, 3154, 276, 22708, 529, 13, 1112, 1659, 7050, 1920, 11, 13265, 7190, 11, 31851, 11, 422, 8624, 185, 185, 1892, 276, 22708, 529, 13, 10518, 185, 185, 3154, 353, 15142, 1659, 353, 15142, 6635, 2714, 185, 185, 185, 2176, 461, 2990, 88, 7190, 7, 6860, 7190, 1772, 185, 185, 315, 1171, 6412, 6310, 185, 315, 972, 1753, 62, 2139, 10942, 185, 436, 967, 7050, 1920, 1497, 11835, 23883, 10042, 4797, 1183, 12036, 77, 4016, 17641, 185, 185, 185, 2176, 461, 2990, 88, 14830, 7, 14830, 11, 461, 2990, 88, 7190, 1772, 185, 185, 315, 972, 3697, 7, 1180, 11, 3076, 11, 1189, 62, 4534, 1772, 185, 436, 1189, 405, 276, 22708, 529, 13, 10518, 13, 703, 62, 2482, 7, 2448, 62, 4534, 8, 185, 185, 436, 1494, 23157, 276, 5439, 3133, 1826, 327, 461, 20950, 10927, 185, 436, 1189, 405, 1189, 13, 3124, 2560, 7, 2448, 13, 15148, 58, 1161, 12, 16, 5589, 185, 185, 436, 365, 1714, 7, 3261, 11, 440, 86, 65, 2456, 372, 267, 25, 185, 655, 21807, 13, 13766, 7, 69, 11, 1189, 8, 185, 185, 185, 2176, 461, 2990, 88, 13105, 7, 13105, 11, 461, 2990, 88, 7190, 1772, 185, 185, 315, 972, 1272, 7, 1180, 11, 3076, 1772, 185, 436, 365, 1714, 7, 3261, 11, 440, 15686, 2456, 372, 267, 25, 185, 655, 1189, 405, 21807, 13, 1768, 7, 69, 8, 185, 185, 436, 562, 10405, 7, 2448, 13, 15148, 8, 2069, 4084, 18, 25, 185, 655, 967, 353, 15142, 6635, 2714, 822, 185, 185, 436, 1494, 461, 20950, 10927, 1189, 372, 5439, 3133, 1826, 13, 627, 51, 42, 27131, 3812, 3133, 1826, 13, 185, 436, 1189, 405, 1189, 13, 3124, 2560, 7, 2448, 13, 15148, 58, 1161, 12, 16, 5589, 185, 185, 436, 3310, 62, 2448, 405, 353, 15142, 6635, 2714, 822, 185, 436, 334, 87, 11, 320, 11, 1670, 8, 405, 1189, 13, 15148, 185, 185, 436, 3310, 62, 2448, 13, 2964, 27219, 7, 15, 11, 207, 15, 11, 207, 15, 8, 185, 436, 3310, 62, 2448, 13, 2964, 7592, 5015, 7, 16, 11, 207, 16, 11, 207, 16, 8, 185, 436, 3310, 62, 2448, 13, 2964, 6500, 289, 7, 15, 11, 1371, 567, 207, 16, 11, 207, 15, 11, 320, 567, 207, 16, 11, 207, 15, 11, 1670, 567, 207, 16, 8, 185, 436, 276, 22708, 529, 13, 10518, 13, 1113, 62, 2482, 7, 5468, 62, 2448, 11, 1189, 8, 185, 185, 436, 967, 3310, 62, 2448, 185, 13518, 21289, 25, 185, 9138, 339, 18537, 327, 291, 4016, 4797, 185, 32021], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9138, 339, 18537, 327, 291, 4016, 4797, 185, 32021].\n",
      "Sample 4 of the training set: <｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
      "### Instruction:\n",
      "You will be given two Python code snippets: one is the original version, and the other is the updated version. Your task is to provide a clear, concise, and accurate short description of the update that was made in the updated version. Now, here are the original and updated code snippets for you to analyze:\n",
      "Original code:\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "###############################################################################\n",
      "# This source file is part of the Tomviz project, https://tomviz.org/.\n",
      "# It is released under the 3-Clause BSD License, see \"LICENSE\".\n",
      "###############################################################################\n",
      "import numpy as np\n",
      "\n",
      "from tomviz.io import FileType, IOBase, Reader, Writer\n",
      "\n",
      "import tomviz.utils\n",
      "\n",
      "from vtk import vtkImageData\n",
      "\n",
      "\n",
      "class NumpyBase(IOBase):\n",
      "\n",
      "    @staticmethod\n",
      "    def file_type():\n",
      "        return FileType('NumPy binary format', ['npy'])\n",
      "\n",
      "\n",
      "class NumpyWriter(Writer, NumpyBase):\n",
      "\n",
      "    def write(self, path, data_object):\n",
      "        data = tomviz.utils.get_array(data_object)\n",
      "        with open(path, \"wb\") as f:\n",
      "            np.save(f, data)\n",
      "\n",
      "\n",
      "class NumpyReader(Reader, NumpyBase):\n",
      "\n",
      "    def read(self, path):\n",
      "        with open(path, \"rb\") as f:\n",
      "            data = np.load(f)\n",
      "\n",
      "        if len(data.shape) != 3:\n",
      "            return vtkImageData()\n",
      "\n",
      "        image_data = vtkImageData()\n",
      "        (x, y, z) = data.shape\n",
      "\n",
      "        image_data.SetOrigin(0, 0, 0)\n",
      "        image_data.SetSpacing(1, 1, 1)\n",
      "        image_data.SetExtent(0, x - 1, 0, y - 1, 0, z - 1)\n",
      "        tomviz.utils.set_array(image_data, data)\n",
      "\n",
      "        return image_data\n",
      "\n",
      "Updated code:\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "###############################################################################\n",
      "# This source file is part of the Tomviz project, https://tomviz.org/.\n",
      "# It is released under the 3-Clause BSD License, see \"LICENSE\".\n",
      "###############################################################################\n",
      "import numpy as np\n",
      "\n",
      "from tomviz.io import FileType, IOBase, Reader, Writer\n",
      "\n",
      "import tomviz.utils\n",
      "\n",
      "from vtk import vtkImageData\n",
      "\n",
      "\n",
      "class NumpyBase(IOBase):\n",
      "\n",
      "    @staticmethod\n",
      "    def file_type():\n",
      "        return FileType('NumPy binary format', ['npy'])\n",
      "\n",
      "\n",
      "class NumpyWriter(Writer, NumpyBase):\n",
      "\n",
      "    def write(self, path, data_object):\n",
      "        data = tomviz.utils.get_array(data_object)\n",
      "\n",
      "        # Switch to row major order for NPY stores\n",
      "        data = data.reshape(data.shape[::-1])\n",
      "\n",
      "        with open(path, \"wb\") as f:\n",
      "            np.save(f, data)\n",
      "\n",
      "\n",
      "class NumpyReader(Reader, NumpyBase):\n",
      "\n",
      "    def read(self, path):\n",
      "        with open(path, \"rb\") as f:\n",
      "            data = np.load(f)\n",
      "\n",
      "        if len(data.shape) != 3:\n",
      "            return vtkImageData()\n",
      "\n",
      "        # NPY stores data as row major order. VTK expects column major order.\n",
      "        data = data.reshape(data.shape[::-1])\n",
      "\n",
      "        image_data = vtkImageData()\n",
      "        (x, y, z) = data.shape\n",
      "\n",
      "        image_data.SetOrigin(0, 0, 0)\n",
      "        image_data.SetSpacing(1, 1, 1)\n",
      "        image_data.SetExtent(0, x - 1, 0, y - 1, 0, z - 1)\n",
      "        tomviz.utils.set_array(image_data, data)\n",
      "\n",
      "        return image_data\n",
      "### Response:\n",
      "Use C ordering for npy format\n",
      "<|EOT|>.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    train_tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size = 1000,\n",
    "    remove_columns=dataset.column_names,\n",
    "    fn_kwargs={ \"tokenizer\": tokenizer }\n",
    ")\n",
    "\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_data = split_dataset['train']\n",
    "valid_data = split_dataset['test']\n",
    "\n",
    "print(train_data)\n",
    "index=4\n",
    "print(f\"Sample {index} of the training set: {train_data[index]['input_ids']}, {train_data[index]['labels']}.\")\n",
    "print(f\"Sample {index} of the training set: {tokenizer.decode(list(train_data[index]['input_ids']))}.\")\n",
    "\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "data_module = dict(train_dataset=train_data, eval_dataset=valid_data, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T11:15:43.527748Z",
     "iopub.status.busy": "2024-08-20T11:15:43.527477Z",
     "iopub.status.idle": "2024-08-20T11:16:39.423077Z",
     "shell.execute_reply": "2024-08-20T11:16:39.422558Z",
     "shell.execute_reply.started": "2024-08-20T11:15:43.527730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f9ca8fc27c49bfaa9c34d4e2a4d145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d331bee33e471082fe35c44d0aad82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12091cf98194e47890c4da48ec8f0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0152b55ec542608168a64465f673a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c438fecd024e438c6b9fda735b76b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73d018bc3674ef0ba925e5d923ae025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3621f7afd748e9bc56408b951471b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=getattr(torch, \"bfloat16\"),\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    load_in_8bit=False,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T11:16:39.424026Z",
     "iopub.status.busy": "2024-08-20T11:16:39.423816Z",
     "iopub.status.idle": "2024-08-20T11:16:39.712161Z",
     "shell.execute_reply": "2024-08-20T11:16:39.711574Z",
     "shell.execute_reply.started": "2024-08-20T11:16:39.423972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k_proj', 'o_proj', 'up_proj', 'down_proj', 'q_proj', 'gate_proj', 'v_proj']\n",
      "trainable params: 19,988,480 || all params: 6,760,501,248 || trainable%: 0.2956656506189288\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model, bit4=True, bit8=True):\n",
    "    LinearModule = torch.nn.Linear\n",
    "    if bit4:\n",
    "        LinearModule = bnb.nn.Linear4bit\n",
    "    if bit8:\n",
    "        LinearModule = bnb.nn.Linear8bitLt\n",
    "        \n",
    "    module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LinearModule):\n",
    "            if \"lm_head\" in module_names:\n",
    "                continue\n",
    "            names = name.split(\".\")\n",
    "            module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    \n",
    "    return list(module_names)\n",
    "\n",
    "target_modules = find_all_linear_names(model, bit4=True, bit8=False)\n",
    "print(target_modules)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T11:16:39.713241Z",
     "iopub.status.busy": "2024-08-20T11:16:39.713021Z",
     "iopub.status.idle": "2024-08-20T11:16:41.335279Z",
     "shell.execute_reply": "2024-08-20T11:16:41.334772Z",
     "shell.execute_reply.started": "2024-08-20T11:16:39.713221Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "# Set up wandb\n",
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "wandb.login(key=\"22943c48738b2f0aa5a6b37af531509b75a16960\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     loss = F.cross_entropy(logits, labels).item()\n",
    "#     return {\"loss\": loss}\n",
    "\n",
    "# def preprocess_logits_for_metrics(logits, labels):\n",
    "#     return logits\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"]=\"llm-finetune\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T11:16:41.336390Z",
     "iopub.status.busy": "2024-08-20T11:16:41.336009Z",
     "iopub.status.idle": "2024-08-20T16:09:23.766316Z",
     "shell.execute_reply": "2024-08-20T16:09:23.765644Z",
     "shell.execute_reply.started": "2024-08-20T11:16:41.336373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamueld\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240820_111642-tek1cz0w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samueld/llm-finetune/runs/tek1cz0w' target=\"_blank\">breezy-gorge-107</a></strong> to <a href='https://wandb.ai/samueld/llm-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samueld/llm-finetune' target=\"_blank\">https://wandb.ai/samueld/llm-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samueld/llm-finetune/runs/tek1cz0w' target=\"_blank\">https://wandb.ai/samueld/llm-finetune/runs/tek1cz0w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 4:52:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.870800</td>\n",
       "      <td>1.889282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.531700</td>\n",
       "      <td>1.960227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.735800</td>\n",
       "      <td>2.431282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>2.964633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.2025971291065216, metrics={'train_runtime': 17558.0866, 'train_samples_per_second': 0.911, 'train_steps_per_second': 0.057, 'total_flos': 6.525737280806584e+17, 'train_loss': 1.2025971291065216, 'epoch': 4.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f2889a58150>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f2930ed6e50, execution_count=9 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f28b0893710, raw_cell=\"from transformers import Trainer, TrainingArgument..\" store_history=True silent=False shell_futures=True cell_id=a3911d8a-6fd5-4f68-9cd7-c0cefdf39f14> result=TrainOutput(global_step=1000, training_loss=1.2025971291065216, metrics={'train_runtime': 17558.0866, 'train_samples_per_second': 0.911, 'train_steps_per_second': 0.057, 'total_flos': 6.525737280806584e+17, 'train_loss': 1.2025971291065216, 'epoch': 4.0})>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "MAX_EPOCHS = 4\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 100\n",
    "LOG_FREQ = 10\n",
    "BATCH_SIZE = 16\n",
    "OUTPUT_DIR = \"deepseek6.7-compare-coder\"\n",
    "SCHEDULER = \"cosine\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"Sam137/{OUTPUT_DIR}\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to = \"wandb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    logging_steps=LOG_FREQ,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.95,\n",
    "    lr_scheduler_type=SCHEDULER,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    push_to_hub=True,\n",
    "    auto_find_batch_size=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, **data_module)\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T16:09:23.768481Z",
     "iopub.status.busy": "2024-08-20T16:09:23.768313Z",
     "iopub.status.idle": "2024-08-20T16:09:35.950951Z",
     "shell.execute_reply": "2024-08-20T16:09:35.950427Z",
     "shell.execute_reply.started": "2024-08-20T16:09:23.768465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f2889a58150>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f287e52f850, raw_cell=\"wandb.finish()\" store_history=True silent=False shell_futures=True cell_id=d7f4d8da-b291-4ec6-9d29-dd4afe879f03>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁▁▅█</td></tr><tr><td>eval/runtime</td><td>▁▁█▂</td></tr><tr><td>eval/samples_per_second</td><td>██▁▇</td></tr><tr><td>eval/steps_per_second</td><td>██▁█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.96463</td></tr><tr><td>eval/runtime</td><td>306.2423</td></tr><tr><td>eval/samples_per_second</td><td>3.265</td></tr><tr><td>eval/steps_per_second</td><td>0.206</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>1000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.318</td></tr><tr><td>train/total_flos</td><td>6.525737280806584e+17</td></tr><tr><td>train/train_loss</td><td>1.2026</td></tr><tr><td>train/train_runtime</td><td>17558.0866</td></tr><tr><td>train/train_samples_per_second</td><td>0.911</td></tr><tr><td>train/train_steps_per_second</td><td>0.057</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-gorge-107</strong> at: <a href='https://wandb.ai/samueld/llm-finetune/runs/tek1cz0w' target=\"_blank\">https://wandb.ai/samueld/llm-finetune/runs/tek1cz0w</a><br/> View job at <a href='https://wandb.ai/samueld/llm-finetune/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjQxMjc0MTU1Nw==/version_details/v2' target=\"_blank\">https://wandb.ai/samueld/llm-finetune/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjQxMjc0MTU1Nw==/version_details/v2</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240820_111642-tek1cz0w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
